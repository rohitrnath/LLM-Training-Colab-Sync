{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxriBsmN1ACKeEi6TMEtea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitrnath/LLM-Training-Colab-Sync/blob/main/OpenCLIP_Model_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zVIapp7Jv5Fu"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from PIL.Image import Image\n",
        "\n",
        "\n",
        "class ClipApp:\n",
        "    \"\"\"\n",
        "    This class consists of light-weight \"app code\" that is required to perform end to end inference with Clip.\n",
        "\n",
        "    The app uses 1 model:\n",
        "        * Clip\n",
        "\n",
        "    For a given image input, the app will:\n",
        "        * pre-process the image\n",
        "        * pre-process the text\n",
        "        * Run Clip inference\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clip_model: torch.nn.Module,\n",
        "    ):\n",
        "        # Open AI Clip\n",
        "        self.text_encoder = clip_model.text_encoder\n",
        "        self.image_encoder = clip_model.image_encoder\n",
        "        # Preprocess Compose function from Open AI clip\n",
        "        self.preprocess = clip_model.preprocess\n",
        "        self.tokenizer = clip_model.tokenizer_func\n",
        "\n",
        "    def predict(self, *args, **kwargs):\n",
        "        # See predict_similarity.\n",
        "        return self.predict_similarity(*args, **kwargs)\n",
        "\n",
        "    def predict_similarity(\n",
        "        self, image: torch.Tensor, text: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            image: torch.Tensor (Shape: [1, 3, 224, 224])\n",
        "                Processed image tensor with values normalized to be between 0-1.\n",
        "            text: torch.Tensor (Shape: [1, 77])\n",
        "                Processed text tensor to be tokenized.\n",
        "\n",
        "        Outputs:\n",
        "            logits_per_image: torch.Tensor (Shape: [num_images, num_text_prompts])\n",
        "\n",
        "                Given a batch of images and a batch of text tokens, returns a tensor,\n",
        "                containing the logit scores corresponding to each image per text input.\n",
        "                The values are cosine similarities between the corresponding image and\n",
        "                text features, times 100. The logits of text per image can be computed\n",
        "                by doing a transpose.\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            image_features = self.image_encoder(image)\n",
        "            text_features = self.text_encoder(text)\n",
        "            logits_per_image = image_features @ text_features.t()\n",
        "        return logits_per_image.cpu().numpy()\n",
        "\n",
        "    def process_image(self, image: Image) -> torch.Tensor:\n",
        "        \"\"\"Process image before calling forward.\n",
        "\n",
        "        Inputs:\n",
        "            image: PIL.Image\n",
        "                Image loaded by Pillow must be provided.\n",
        "                Example: image = Image.open('<path>')\n",
        "\n",
        "        Outputs:\n",
        "            processed_image: torch.Tensor (shape [1, 3, 224, 224])\n",
        "                Layout: RGB\n",
        "                The image is converted to torch tensor and normalized\n",
        "                to be in the range of 0-1.\n",
        "        \"\"\"\n",
        "        return self.preprocess(image).unsqueeze(0)\n",
        "\n",
        "    def process_text(self, text: str) -> torch.Tensor:\n",
        "        \"\"\"Process text into tokens for forward call.\n",
        "\n",
        "        Input:\n",
        "            text: str\n",
        "                Text prompt intended for inference.\n",
        "                Example: \"golden hour\"\n",
        "\n",
        "        Output:\n",
        "            tokenized_tensor: torch.Tensor (shape: [1, 77])\n",
        "            Example: tensor([[49406,  3878,  2232, 49407, 0, 0...]])\n",
        "\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        image_size: Tuple[int, int] = (224, 224),\n",
        "        text_size: Tuple[int, int] = (3, 77),\n",
        "    ):\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        if isinstance(image_size, int):\n",
        "            image_size = (image_size, image_size)\n",
        "        return {\n",
        "            \"image\": ((1, 3, *image_size), \"float32\"),\n",
        "            \"text\": (text_size, \"int32\"),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from qai_hub_models.utils.asset_loaders import SourceAsRoot, callback_with_retry\n",
        "from qai_hub_models.utils.base_model import BaseModel, CollectionModel\n",
        "from qai_hub_models.utils.input_spec import InputSpec\n",
        "\n",
        "PRETRAINED_WEIGHTS = \"ViT-B/16\"\n",
        "MODEL_ID = __name__.split(\".\")[-2]\n",
        "MODEL_ASSET_VERSION = 1\n",
        "OPENAI_CLIP_SOURCE_REPOSITORY = \"https://github.com/openai/CLIP\"\n",
        "OPENAI_CLIP_SOURCE_REPO_COMMIT = \"a1d071733d7111c9c014f024669f959182114e33\"\n",
        "\n",
        "\n",
        "def load_clip_and_tokenizer():\n",
        "    \"\"\"Downloading pretrained weights via OpenAI and loading them.\"\"\"\n",
        "    with SourceAsRoot(\n",
        "        OPENAI_CLIP_SOURCE_REPOSITORY,\n",
        "        OPENAI_CLIP_SOURCE_REPO_COMMIT,\n",
        "        MODEL_ID,\n",
        "        MODEL_ASSET_VERSION,\n",
        "    ):\n",
        "        import clip\n",
        "\n",
        "        tokenizer_func = clip.tokenize\n",
        "        net, preprocess = clip.load(PRETRAINED_WEIGHTS)\n",
        "        return net, preprocess, tokenizer_func\n",
        "\n",
        "\n",
        "class Clip(CollectionModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        text_encoder: torch.nn.Module,\n",
        "        image_encoder: torch.nn.Module,\n",
        "        preprocess: torchvision.transforms.transforms.Compose,\n",
        "        tokenizer_func: Callable,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.text_encoder = text_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "        self.preprocess = preprocess\n",
        "        self.tokenizer_func = tokenizer_func\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pretrained():\n",
        "        net, preprocess, tokenizer_func = callback_with_retry(\n",
        "            num_retries=5, callback=load_clip_and_tokenizer\n",
        "        )\n",
        "        return Clip.from_source_model(net, preprocess, tokenizer_func)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_source_model(net, preprocess, tokenizer_func):\n",
        "        net = net.eval()\n",
        "        text_encoder = ClipTextEncoder(net)\n",
        "        image_encoder = ClipImageEncoder(net)\n",
        "        return Clip(text_encoder, image_encoder, preprocess, tokenizer_func)\n",
        "\n",
        "\n",
        "class ClipTextEncoder(BaseModel):\n",
        "    def __init__(self, net: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        \"\"\" Wrapper for OpenAI CLIP.\"\"\"\n",
        "        self.net = net\n",
        "        self.eot_token = 49407\n",
        "\n",
        "    def forward(self, text: torch.Tensor):\n",
        "        \"\"\"Forward call on Open AI CLIP model.\n",
        "\n",
        "        Inputs:\n",
        "            text: torch.Tensor (Shape: [1, 77] context_length=77)\n",
        "                Processed text tensor to be tokenized.\n",
        "\n",
        "        Outputs:\n",
        "            text_features: torch.Tensor [512 (transformer_width), num_text_prompts]\n",
        "                Raw text features are returned. When multiplied to image features,\n",
        "                you can obtain a matrix of cosine similarities between the\n",
        "                corresponding image and text input.\n",
        "\n",
        "        \"\"\"\n",
        "        clipped_text = torch.clip(text, min=0, max=self.eot_token)\n",
        "        text_features = self.net.encode_text(clipped_text)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "        return text_features\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        batch_size: int = 1,\n",
        "        text_length: int = 77,\n",
        "    ) -> InputSpec:\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        return {\n",
        "            \"text\": ((batch_size, text_length), \"int32\"),\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls):\n",
        "        return Clip.from_pretrained().text_encoder\n",
        "\n",
        "\n",
        "class ClipImageEncoder(BaseModel):\n",
        "    def __init__(self, net: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        \"\"\" Wrapper for OpenAI Clip.\"\"\"\n",
        "        self.net = net\n",
        "        self.eot_token = 49407\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        \"\"\"Forward call on Open AI Clip model.\n",
        "\n",
        "        Inputs:\n",
        "            image: torch.Tensor (Shape: [1, 3, 224, 224])\n",
        "                Processed image tensor with values normalized to be between 0-1.\n",
        "                Channel Layout: RGB\n",
        "\n",
        "        Outputs:\n",
        "            image_features: torch.Tensor [num_images, 512 (transformer_width)]\n",
        "                Raw image features (multiplied to 100) are returned.\n",
        "                When multiplied to text features, you can obtain a\n",
        "                matrix of cosine similarities between the corresponding image and\n",
        "                text input.\n",
        "\n",
        "        \"\"\"\n",
        "        image_features = self.net.encode_image(image)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        return self.net.logit_scale.exp() * image_features\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        height: int = 224,\n",
        "        width: int = 224,\n",
        "    ) -> InputSpec:\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        return {\n",
        "            \"image\": ((1, 3, height, width), \"float32\"),\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls):\n",
        "        return Clip.from_pretrained().image_encoder"
      ],
      "metadata": {
        "id": "XFVctOVqy53L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "def add_output_dir_arg(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        \"-o\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"If specified, saves demo output (e.g. image) to this directory instead of displaying.\",\n",
        "    )\n",
        "    return parser"
      ],
      "metadata": {
        "id": "f-xly7zqxQmp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from qai_hub_models.models.openai_clip.model import  Clip\n",
        "from qai_hub_models.utils.display import display_or_save_image\n",
        "\n",
        "\n",
        "# Run Clip on a directory of images with a query text.\n",
        "# The demo will display similarity score for each image.\n",
        "def main(is_test: bool = False):\n",
        "    # Demo parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--image-dir\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Path to image directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--image-names\",\n",
        "        type=str,\n",
        "        default=\"image1.jpg,image2.jpg,image3.jpg\",\n",
        "        help=\"Specify names of the images in the folder.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--text\",\n",
        "        type=str,\n",
        "        default=\"camping under the stars\",\n",
        "        help=\"Text prompt for image search\",\n",
        "    )\n",
        "    add_output_dir_arg(parser)\n",
        "    args = parser.parse_args([] if is_test else None)\n",
        "\n",
        "    # Load model\n",
        "    clip_model = Clip.from_pretrained()\n",
        "    app = ClipApp(clip_model=clip_model)\n",
        "\n",
        "    image_names = args.image_names.split(\",\")\n",
        "    text = app.process_text(args.text)\n",
        "    images = []\n",
        "\n",
        "    # Iterate through images and text provided by user\n",
        "    for filename in image_names:\n",
        "        # Make sure the file is an image\n",
        "        if os.path.splitext(filename)[1].lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            if args.image_dir:\n",
        "                image = os.path.join(args.image_dir, filename)\n",
        "            # Preprocess image and text pair\n",
        "            image = app.process_image(Image.open(image))\n",
        "            images.append(image)\n",
        "\n",
        "        else:\n",
        "            print(f\"Skipping file {filename}\")\n",
        "\n",
        "    images = torch.stack(images).squeeze(1)\n",
        "\n",
        "    # Compute similarity\n",
        "    predictions = app.predict_similarity(images, text).flatten()\n",
        "\n",
        "    # Display all the images and their score wrt to the text prompt provided.\n",
        "    print(f\"Searching images by prompt: {args.text}\")\n",
        "    for i in range(len(predictions)):\n",
        "        print(\n",
        "            f\"\\t Image with name: {image_names[i]} has a similarity score={predictions[i]}\"\n",
        "        )\n",
        "\n",
        "    # Show image\n",
        "    print(\"Displaying the most relevant image\")\n",
        "\n",
        "    selected_image = image_names[np.argmax(predictions)]\n",
        "    if args.image_dir:\n",
        "        selected_image = os.path.join(args.image_dir, selected_image)\n",
        "    most_relevant_image = Image.open(selected_image)\n",
        "\n",
        "    if not is_test:\n",
        "        display_or_save_image(most_relevant_image, args.output_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FQUbmzBQv_C-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}