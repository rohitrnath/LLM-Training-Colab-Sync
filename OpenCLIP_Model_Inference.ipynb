{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP75s4yfQlLlL5Gm3nggSKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitrnath/LLM-Training-Colab-Sync/blob/main/OpenCLIP_Model_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id4XB8W34Gwz",
        "outputId": "888b06e2-af50-4d51-f63a-1f3daea0cb2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m942.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/CLIP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe3q4GIX1gu3",
        "outputId": "30d88fbc-1014-43fc-f266-cfebab745c4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 251, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 251 (delta 3), reused 3 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (251/251), 8.93 MiB | 21.51 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CLIP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4QnpXcE1kqH",
        "outputId": "e41bd002-14c8-41c0-8f69-fb45f8e2b292"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIP/clip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "class ClipTextEncoder(torch.nn.Module):\n",
        "    def __init__(self, net: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        \"\"\" Wrapper for OpenAI CLIP.\"\"\"\n",
        "        self.net = net\n",
        "        self.eot_token = 49407\n",
        "\n",
        "    def forward(self, text: torch.Tensor):\n",
        "        \"\"\"Forward call on Open AI CLIP model.\n",
        "\n",
        "        Inputs:\n",
        "            text: torch.Tensor (Shape: [1, 77] context_length=77)\n",
        "                Processed text tensor to be tokenized.\n",
        "\n",
        "        Outputs:\n",
        "            text_features: torch.Tensor [512 (transformer_width), num_text_prompts]\n",
        "                Raw text features are returned. When multiplied to image features,\n",
        "                you can obtain a matrix of cosine similarities between the\n",
        "                corresponding image and text input.\n",
        "\n",
        "        \"\"\"\n",
        "        clipped_text = torch.clip(text, min=0, max=self.eot_token)\n",
        "        text_features = self.net.encode_text(clipped_text)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "        return text_features\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        batch_size: int = 1,\n",
        "        text_length: int = 77,\n",
        "    ) :\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        return {\n",
        "            \"text\": ((batch_size, text_length), \"int32\"),\n",
        "        }\n",
        "\n",
        "\n",
        "class ClipImageEncoder(torch.nn.Module):\n",
        "    def __init__(self, net: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        \"\"\" Wrapper for OpenAI Clip.\"\"\"\n",
        "        self.net = net\n",
        "        self.eot_token = 49407\n",
        "\n",
        "    def forward(self, image: torch.Tensor):\n",
        "        \"\"\"Forward call on Open AI Clip model.\n",
        "\n",
        "        Inputs:\n",
        "            image: torch.Tensor (Shape: [1, 3, 224, 224])\n",
        "                Processed image tensor with values normalized to be between 0-1.\n",
        "                Channel Layout: RGB\n",
        "\n",
        "        Outputs:\n",
        "            image_features: torch.Tensor [num_images, 512 (transformer_width)]\n",
        "                Raw image features (multiplied to 100) are returned.\n",
        "                When multiplied to text features, you can obtain a\n",
        "                matrix of cosine similarities between the corresponding image and\n",
        "                text input.\n",
        "\n",
        "        \"\"\"\n",
        "        image_features = self.net.encode_image(image)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        return self.net.logit_scale.exp() * image_features\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        height: int = 224,\n",
        "        width: int = 224,\n",
        "    ) :\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        return {\n",
        "            \"image\": ((1, 3, height, width), \"float32\"),\n",
        "        }"
      ],
      "metadata": {
        "id": "XFVctOVqy53L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "zVIapp7Jv5Fu"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class ClipApp:\n",
        "    \"\"\"\n",
        "    This class consists of light-weight \"app code\" that is required to perform end to end inference with Clip.\n",
        "\n",
        "    The app uses 1 model:\n",
        "        * Clip\n",
        "\n",
        "    For a given image input, the app will:\n",
        "        * pre-process the image\n",
        "        * pre-process the text\n",
        "        * Run Clip inference\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        net: torch.nn.Module,\n",
        "        preprocess: torchvision.transforms.transforms.Compose,\n",
        "        tokenizer_func: Callable,\n",
        "    ):\n",
        "        # Open AI Clip\n",
        "        self.text_encoder = ClipTextEncoder(net)\n",
        "        self.image_encoder = ClipImageEncoder(net)\n",
        "        # Preprocess Compose function from Open AI clip\n",
        "        self.preprocess = preprocess\n",
        "        self.tokenizer = tokenizer_func\n",
        "\n",
        "    def predict(self, *args, **kwargs):\n",
        "        # See predict_similarity.\n",
        "        return self.predict_similarity(*args, **kwargs)\n",
        "\n",
        "    def predict_similarity(\n",
        "        self, image: torch.Tensor, text: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            image: torch.Tensor (Shape: [1, 3, 224, 224])\n",
        "                Processed image tensor with values normalized to be between 0-1.\n",
        "            text: torch.Tensor (Shape: [1, 77])\n",
        "                Processed text tensor to be tokenized.\n",
        "\n",
        "        Outputs:\n",
        "            logits_per_image: torch.Tensor (Shape: [num_images, num_text_prompts])\n",
        "\n",
        "                Given a batch of images and a batch of text tokens, returns a tensor,\n",
        "                containing the logit scores corresponding to each image per text input.\n",
        "                The values are cosine similarities between the corresponding image and\n",
        "                text features, times 100. The logits of text per image can be computed\n",
        "                by doing a transpose.\n",
        "\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            image_features = self.image_encoder(image)\n",
        "            text_features = self.text_encoder(text)\n",
        "            logits_per_image = image_features @ text_features.t()\n",
        "        return logits_per_image.cpu().numpy()\n",
        "\n",
        "    def process_image(self, image: Image) -> torch.Tensor:\n",
        "        \"\"\"Process image before calling forward.\n",
        "\n",
        "        Inputs:\n",
        "            image: PIL.Image\n",
        "                Image loaded by Pillow must be provided.\n",
        "                Example: image = Image.open('<path>')\n",
        "\n",
        "        Outputs:\n",
        "            processed_image: torch.Tensor (shape [1, 3, 224, 224])\n",
        "                Layout: RGB\n",
        "                The image is converted to torch tensor and normalized\n",
        "                to be in the range of 0-1.\n",
        "        \"\"\"\n",
        "        return self.preprocess(image).unsqueeze(0)\n",
        "\n",
        "    def process_text(self, text: str) -> torch.Tensor:\n",
        "        \"\"\"Process text into tokens for forward call.\n",
        "\n",
        "        Input:\n",
        "            text: str\n",
        "                Text prompt intended for inference.\n",
        "                Example: \"golden hour\"\n",
        "\n",
        "        Output:\n",
        "            tokenized_tensor: torch.Tensor (shape: [1, 77])\n",
        "            Example: tensor([[49406,  3878,  2232, 49407, 0, 0...]])\n",
        "\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def get_input_spec(\n",
        "        self,\n",
        "        image_size: Tuple[int, int] = (224, 224),\n",
        "        text_size: Tuple[int, int] = (3, 77),\n",
        "    ):\n",
        "        # Get the input specification ordered (name -> (shape, type)) pairs for this model.\n",
        "        #\n",
        "        # This can be used with the qai_hub python API to declare\n",
        "        # the model input specification upon submitting a profile job.\n",
        "        if isinstance(image_size, int):\n",
        "            image_size = (image_size, image_size)\n",
        "        return {\n",
        "            \"image\": ((1, 3, *image_size), \"float32\"),\n",
        "            \"text\": (text_size, \"int32\"),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "PRETRAINED_WEIGHTS = \"ViT-B/16\"\n",
        "tokenizer_func = clip.tokenize\n",
        "net, preprocess = clip.load(PRETRAINED_WEIGHTS)"
      ],
      "metadata": {
        "id": "irTTpuCt3LYs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Run Clip on a directory of images with a query text.\n",
        "# The demo will display similarity score for each image.\n",
        "def main(is_test: bool = False):\n",
        "    # Demo parameters\n",
        "    text_str = \"eye retina operated image\"\n",
        "    image_dir = \"/content\"\n",
        "    image_names = \"image1.jpg,image2.jpg,image3.jpg\"\n",
        "    # Load model\n",
        "    app = ClipApp(net, preprocess, tokenizer_func)\n",
        "\n",
        "    image_names = image_names.split(\",\")\n",
        "    text = app.process_text(text_str)\n",
        "    images = []\n",
        "\n",
        "    # Iterate through images and text provided by user\n",
        "    for filename in image_names:\n",
        "        # Make sure the file is an image\n",
        "        if os.path.splitext(filename)[1].lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "            if image_dir:\n",
        "                image = os.path.join(image_dir, filename)\n",
        "            # Preprocess image and text pair\n",
        "            print(image)\n",
        "            image = app.process_image(Image.open(image))\n",
        "            images.append(image)\n",
        "\n",
        "        else:\n",
        "            print(f\"Skipping file {filename}\")\n",
        "\n",
        "    images = torch.stack(images).squeeze(1)\n",
        "\n",
        "    # Compute similarity\n",
        "    predictions = app.predict_similarity(images, text).flatten()\n",
        "\n",
        "    # Display all the images and their score wrt to the text prompt provided.\n",
        "    print(f\"Searching images by prompt: {text}\")\n",
        "    for i in range(len(predictions)):\n",
        "        print(\n",
        "            f\"\\t Image with name: {image_names[i]} has a similarity score={predictions[i]}\"\n",
        "        )\n",
        "\n",
        "    # Show image\n",
        "    print(\"Displaying the most relevant image\")\n",
        "\n",
        "    print(np.argmax(predictions))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQUbmzBQv_C-",
        "outputId": "bc6491e3-1543-46cd-d9ac-27c572afa8f4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/image1.jpg\n",
            "/content/image2.jpg\n",
            "/content/image3.jpg\n",
            "Searching images by prompt: tensor([[49406,  3272, 36919, 23031,  2867, 49407,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
            "\t Image with name: image1.jpg has a similarity score=26.78359031677246\n",
            "\t Image with name: image2.jpg has a similarity score=19.889570236206055\n",
            "\t Image with name: image3.jpg has a similarity score=21.584095001220703\n",
            "Displaying the most relevant image\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example given by OpenAI"
      ],
      "metadata": {
        "id": "jQLWtqY-H43u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image = preprocess(Image.open(\"/content/image1.jpg\")).unsqueeze(0).to(device)\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"eye retina operated image\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSvOFiooC4_N",
        "outputId": "790292ac-7aad-49b0-8d50-c3d5776b922c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: [[7.2572782e-04 4.6486195e-04 9.9880946e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HA_ORHZOGomM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}